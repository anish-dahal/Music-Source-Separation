{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm\n",
    "\n",
    "from CustomDataset.Data import (\n",
    "    DSD100,\n",
    "    data_split,\n",
    "    dataloader\n",
    ")\n",
    "from model.Network import UNet\n",
    "from model.train_model import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2472519fc70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DSD100('../DSD100spectrogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 511, 127])\n",
      "torch.Size([1, 511, 127])\n",
      "torch.Size([1, 511, 127])\n",
      "torch.Size([1, 511, 127])\n",
      "torch.Size([1, 511, 127])\n"
     ]
    }
   ],
   "source": [
    "for mixture, bass, drum, vocal, instrumental in dataset:\n",
    "    print(mixture.shape)\n",
    "    print(bass.shape)\n",
    "    print(drum.shape)\n",
    "    print(vocal.shape)\n",
    "    print(instrumental.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10284"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = data_split(dataset, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7199"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3085"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = dataloader(train_dataset, 16, shuffle = True)\n",
    "val_dataloader = dataloader(val_dataset, 16, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 511, 127])\n",
      "torch.Size([16, 1, 511, 127])\n",
      "torch.Size([16, 1, 511, 127])\n",
      "torch.Size([16, 1, 511, 127])\n",
      "torch.Size([16, 1, 511, 127])\n"
     ]
    }
   ],
   "source": [
    "for mixture, bass, drum, vocal, instrumental in train_dataloader:\n",
    "    print(mixture.shape)\n",
    "    print(bass.shape)\n",
    "    print(drum.shape)\n",
    "    print(vocal.shape)\n",
    "    print(instrumental.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = UNet().to(device)\n",
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [16, 16, 255, 63]             416\n",
      "       BatchNorm2d-2          [16, 16, 255, 63]              32\n",
      "         LeakyReLU-3          [16, 16, 255, 63]               0\n",
      "           encoder-4          [16, 16, 255, 63]               0\n",
      "            Conv2d-5          [16, 32, 127, 31]          12,832\n",
      "       BatchNorm2d-6          [16, 32, 127, 31]              64\n",
      "         LeakyReLU-7          [16, 32, 127, 31]               0\n",
      "           encoder-8          [16, 32, 127, 31]               0\n",
      "            Conv2d-9           [16, 64, 63, 15]          51,264\n",
      "      BatchNorm2d-10           [16, 64, 63, 15]             128\n",
      "        LeakyReLU-11           [16, 64, 63, 15]               0\n",
      "          encoder-12           [16, 64, 63, 15]               0\n",
      "           Conv2d-13           [16, 128, 31, 7]         204,928\n",
      "      BatchNorm2d-14           [16, 128, 31, 7]             256\n",
      "        LeakyReLU-15           [16, 128, 31, 7]               0\n",
      "          encoder-16           [16, 128, 31, 7]               0\n",
      "           Conv2d-17           [16, 256, 15, 3]         819,456\n",
      "      BatchNorm2d-18           [16, 256, 15, 3]             512\n",
      "        LeakyReLU-19           [16, 256, 15, 3]               0\n",
      "          encoder-20           [16, 256, 15, 3]               0\n",
      "           Conv2d-21            [16, 512, 7, 1]       3,277,312\n",
      "      BatchNorm2d-22            [16, 512, 7, 1]           1,024\n",
      "        LeakyReLU-23            [16, 512, 7, 1]               0\n",
      "          encoder-24            [16, 512, 7, 1]               0\n",
      "  ConvTranspose2d-25           [16, 256, 15, 3]       3,277,056\n",
      "      BatchNorm2d-26           [16, 256, 15, 3]             512\n",
      "          decoder-27           [16, 256, 15, 3]               0\n",
      "          Dropout-28           [16, 256, 15, 3]               0\n",
      "             ReLU-29           [16, 256, 15, 3]               0\n",
      "  ConvTranspose2d-30           [16, 128, 31, 7]       1,638,528\n",
      "      BatchNorm2d-31           [16, 128, 31, 7]             256\n",
      "          decoder-32           [16, 128, 31, 7]               0\n",
      "          Dropout-33           [16, 128, 31, 7]               0\n",
      "             ReLU-34           [16, 128, 31, 7]               0\n",
      "  ConvTranspose2d-35           [16, 64, 63, 15]         409,664\n",
      "      BatchNorm2d-36           [16, 64, 63, 15]             128\n",
      "          decoder-37           [16, 64, 63, 15]               0\n",
      "          Dropout-38           [16, 64, 63, 15]               0\n",
      "             ReLU-39           [16, 64, 63, 15]               0\n",
      "  ConvTranspose2d-40          [16, 32, 127, 31]         102,432\n",
      "      BatchNorm2d-41          [16, 32, 127, 31]              64\n",
      "          decoder-42          [16, 32, 127, 31]               0\n",
      "             ReLU-43          [16, 32, 127, 31]               0\n",
      "  ConvTranspose2d-44          [16, 16, 255, 63]          25,616\n",
      "      BatchNorm2d-45          [16, 16, 255, 63]              32\n",
      "          decoder-46          [16, 16, 255, 63]               0\n",
      "             ReLU-47          [16, 16, 255, 63]               0\n",
      "  ConvTranspose2d-48          [16, 1, 511, 127]             801\n",
      "      BatchNorm2d-49          [16, 1, 511, 127]               2\n",
      "          decoder-50          [16, 1, 511, 127]               0\n",
      "          Sigmoid-51          [16, 1, 511, 127]               0\n",
      "================================================================\n",
      "Total params: 9,823,315\n",
      "Trainable params: 9,823,315\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.96\n",
      "Forward/backward pass size (MB): 517.10\n",
      "Params size (MB): 37.47\n",
      "Estimated Total Size (MB): 558.54\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.to(device), input_size = (1, 511, 127), batch_size = 16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n",
    "- use L1 loss(MAE) as loss function\n",
    "- use euclidean distance as evaluation metrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distace(true_value, predicted_value):\n",
    "    return torch.sqrt(torch.sum(torch.square(true_value-predicted_value)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 450/450 [22:52<00:00,  3.05s/it, train_euclidean_distance=29, train_loss=0.397]  \n",
      "100%|██████████| 193/193 [08:54<00:00,  2.77s/it, train_euclidean_distance=29, train_loss=0.397, val_euclidean_distance=28.6, val_loss=0.337]\n",
      "Epoch 1: 100%|██████████| 450/450 [13:27<00:00,  1.80s/it, train_euclidean_distance=29.2, train_loss=0.29] \n",
      "100%|██████████| 193/193 [04:40<00:00,  1.45s/it, train_euclidean_distance=29.2, train_loss=0.29, val_euclidean_distance=28.9, val_loss=0.247]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "training_loss = []\n",
    "validation_loss = []\n",
    "training_distance = []\n",
    "validation_distance = []\n",
    "epoch = 2\n",
    "for i in range(epoch):\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    train_distance = []\n",
    "    val_distance = []\n",
    "    train_loop = tqdm(train_dataloader, leave = True)\n",
    "    for mixture, _, _, vocal, _ in train_loop:\n",
    "        train_loop.set_description(f\"Epoch {i}\")\n",
    "        optimizer.zero_grad()\n",
    "        y = model(mixture)\n",
    "        loss = loss_fn(vocal, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # pred = torch.mul(mixture, y)\n",
    "        distance = euclidean_distace(vocal, y)\n",
    "        train_loss.append(loss.item())\n",
    "        train_distance.append(distance.item())\n",
    "\n",
    "        train_loop.set_postfix(\n",
    "            train_loss = sum(train_loss)/len(train_loss),\n",
    "            train_euclidean_distance = sum(train_distance)/len(train_distance)\n",
    "        )\n",
    "    \n",
    "    val_loop = tqdm(val_dataloader, leave = True)\n",
    "    with torch.no_grad():\n",
    "        for mixture, _, _, vocal, _ in val_loop:\n",
    "            y = model(mixture)\n",
    "            loss = loss_fn(vocal, y)\n",
    "\n",
    "            # pred = torch.mul(mixture, y)\n",
    "            distance = euclidean_distace(vocal, y)\n",
    "            val_loss.append(loss.item())\n",
    "            val_distance.append(distance.item())\n",
    "\n",
    "            val_loop.set_postfix(\n",
    "                train_loss = sum(train_loss)/len(train_loss),\n",
    "                train_euclidean_distance = sum(train_distance)/len(train_distance),\n",
    "                val_loss = sum(val_loss)/len(val_loss),\n",
    "                val_euclidean_distance = sum(val_distance)/len(val_distance)\n",
    "            )\n",
    "    \n",
    "    training_loss.append(sum(train_loss)/len(train_loss))\n",
    "    training_distance.append(sum(train_distance)/len(train_distance))\n",
    "    validation_loss.append(sum(val_loss)/len(val_loss))\n",
    "    validation_distance.append(sum(val_distance)/len(val_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'./model/save_model/vocal.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train using py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 16/16 [00:40<00:00,  2.56s/it, train_euclidean_distance=15.5, train_loss=0.45] \n",
      "100%|██████████| 7/7 [00:11<00:00,  1.65s/it, train_euclidean_distance=15.5, train_loss=0.45, val_euclidean_distance=30.4, val_loss=0.447]\n"
     ]
    }
   ],
   "source": [
    "history = train(model, train_dataloader, val_dataloader, \"./model/save_model/\", stem_name = \"vocal\", epoch = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.45759459026157856, 0.45010572113096714],\n",
       " [28.555444836616516, 15.460571944713593],\n",
       " [0.45242754902158466, 0.4471405787127359],\n",
       " [30.338063648768834, 30.36661638532366])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
